{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import vgg19\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained VGG19 model\n",
    "model = vgg19(pretrained=True).features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the style transfer function\n",
    "def style_transfer(input_image, style):\n",
    "    # Define image transformations\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        T.Lambda(lambda x: x.unsqueeze(0)),\n",
    "    ])\n",
    "    device = 'cpu'\n",
    "    \n",
    "    # # Apply transformations\n",
    "    # print(type(input_image))\n",
    "    # print(input_image.shape)\n",
    "    input_image = Image.fromarray(input_image)\n",
    "    input_tensor = transform(input_image).to(device)\n",
    "\n",
    "    # Style image URL dictionary\n",
    "    style_urls = {\n",
    "        \"Monet\": \"https://4.bp.blogspot.com/-7p47RxU6vjE/UoyAGx7WTxI/AAAAAAAApyY/tiECyoT6okU/s1600/1899-1900+The+Iris+Garden+at+Giverny+oil+on+canvas+Yale+89.5+x+92.1+cm+University+Art+Gallery%252C+New+Haven+CT.jpg\",\n",
    "        \"sakura\": \"https://i2.kknews.cc/SIG=1cjibd3/ctp-vzntr/1528451006916p490op5297.jpg\",\n",
    "        \"sky\": \"https://tse2.mm.bing.net/th?id=OIP.g0-Xd7agO_cy-P0zAeIA-gHaEo&pid=Api&P=0&h=180\",\n",
    "    }\n",
    "\n",
    "    # Load and transform the style image\n",
    "    style_image = Image.open(requests.get(style_urls[style], stream=True).raw)\n",
    "    style_tensor = transform(style_image).to(device)\n",
    "\n",
    "    w_min = min(input_tensor.shape[2], style_tensor.shape[2])\n",
    "    h_min = min(input_tensor.shape[3], style_tensor.shape[3])\n",
    "    input_tensor = input_tensor[:, :, :w_min, :h_min]\n",
    "    style_tensor = style_tensor[:, :, :w_min, :h_min]\n",
    "\n",
    "    print(style_tensor.shape, input_tensor.shape)\n",
    "\n",
    "    # # Simple style transfer (this is a placeholder for an actual style transfer algorithm)\n",
    "    # with torch.no_grad():\n",
    "    #     styled_tensor = model(input_tensor + style_tensor)  # Simulate a combination of features\n",
    "    styled_tensor = input_tensor * 0.7 + style_tensor *0.4\n",
    "\n",
    "    print(styled_tensor.shape)\n",
    "    # # Remove the normalization to display the image correctly\n",
    "    styled_tensor = styled_tensor.squeeze(0).cpu().detach()\n",
    "    styled_tensor = styled_tensor.mul(torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)).add(torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1))\n",
    "    styled_image = T.ToPILImage()(styled_tensor)\n",
    "\n",
    "    return styled_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-openai-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
